# walteraRagConsultationDocuments

**ID**: `8nY0bwCpdQumQMaL`
**Statut**: ‚ö†Ô∏è **OBSOL√àTE** - Remplac√© par Edge Function `recherche-contrats`
**Cr√©√© le**: 02/04/2025
**Derni√®re mise √† jour**: 18/08/2025
**Date de migration**: 15/01/2026

---

## üìã Description

Agent conversationnel RAG (Retrieval-Augmented Generation) pour la recherche dans les documents clients WALTERA (100 000+ documents). Utilise OpenAI GPT-4.1 Mini avec recherche vectorielle Supabase.

**‚ö†Ô∏è Ce workflow a √©t√© remplac√© par l'Edge Function Supabase `recherche-contrats`** pour de meilleures performances et une meilleure qualit√© de r√©sultats.

---

## üéØ Objectif

Permettre aux utilisateurs d'interroger la base documentaire clients de WALTERA (contrats, conditions g√©n√©rales, avenants, etc.) via une interface conversationnelle intelligente.

---

## üîÑ Triggers

### 1. **Chat Trigger** (Chat public)
- **Type** : Trigger chat n8n
- **Acc√®s** : Public (option `public: true`)
- **Webhook ID** : `waltera-chat`
- **Format input** :
```json
{
  "inputs": {
    "chatInput": "Quelle est la r√©mun√©ration dans le contrat aureaPrimaCps de GARED ?"
  }
}
```

### 2. **Webhook1** (HTTP POST)
- **Path de production** : `/walteraClients`
- **Path de test** : `/webhook-test/c0db56ae-5e75-459f-8094-f458084b4e2b`
- **M√©thode** : POST

**Format de la requ√™te** :
```json
{
  "body": {
    "chatInput": "Garanties sant√© FINARE",
    "sessionId": "user-session-123"
  }
}
```

---

## üèóÔ∏è Architecture du Workflow

### Phase 1 : R√©ception et Normalisation

#### **Node: When chat message received**
- Trigger chat public
- Re√ßoit `chatInput` de l'utilisateur

#### **Node: Webhook1**
- Trigger webhook pour int√©grations externes
- Support de `sessionId` personnalis√©

#### **Node: Edit Fields**
- Normalise les inputs (chat ou webhook)
- Extrait `chatInput` et `sessionId`

**Logique de sessionId** :
```javascript
sessionId = $json?.sessionId
         || $json.body?.sessionId
         || $json.query?.sessionId
         || 'default-' + $now.format('yyyy-MM-dd')
```

---

### Phase 2 : Agent RAG Principal

#### **Node: Agent RAG WALTERA**
- **Type** : Agent LangChain
- **Mod√®le** : OpenAI GPT-4.1 Mini (temp√©rature 0.1)
- **M√©moire** : Simple Memory (10 messages) - **D√âSACTIV√âE**

**System Prompt** :
```
Tu es l'assistant IA de WALTERA, courtier en assurance sp√©cialis√© en mutuelles, pr√©voyance et retraite.

## Contexte
Tu as acc√®s √† une base de 100 000+ documents clients stock√©s dans Supabase.
Chaque document appartient √† un client identifi√© par un code (G0001, G0002...)
et peut √™tre li√© √† des conventions collectives (IDCC).

## Outils

**documents** : Recherche s√©mantique
- Retourne les chunks les plus similaires √† ta requ√™te
- IMPORTANT : Sois tr√®s sp√©cifique dans tes recherches

**detailLigne** : Requ√™tes SQL sur donn√©es tabulaires
- Pour calculs et analyses chiffr√©es

**detailDocument** : Contenu complet d'un document
- Param√®tre : file_id
- Utilise apr√®s avoir identifi√© le bon document

**rechercheClient** ‚Üí Recherche filtr√©e par client (PRIORITAIRE)
- Param√®tres : client_name, query
- Utilise TOUJOURS cet outil quand un client est mentionn√©
- Exemple : client_name="GARED", query="aureaPrimaCps r√©mun√©ration"

## Strat√©gie de recherche (CRITIQUE)

La base contient beaucoup de documents. Pour trouver la bonne information :

1. **Inclus TOUJOURS dans ta requ√™te** :
   - Le nom du client (GARED, ALTESSE, FINARE...)
   - Le nom du document si mentionn√© (aureaPrimaCps, Conditions g√©n√©rales...)
   - Les mots-cl√©s sp√©cifiques de la question

2. **Exemples de bonnes requ√™tes** :
   - Question : "Quelle est la r√©mun√©ration dans le contrat aureaPrimaCps de GARED ?"
   - Requ√™te : "GARED aureaPrimaCps r√©mun√©ration prestataire article 6"

   - Question : "Quelles sont les garanties sant√© de FINARE ?"
   - Requ√™te : "FINARE garanties sant√© conditions g√©n√©rales"

3. **Si la premi√®re recherche √©choue** :
   - Reformule avec des synonymes
   - Essaie avec le nom exact du fichier
   - Cherche des termes plus g√©n√©riques puis affine

## Format de r√©ponse

R√©ponds de mani√®re concise et directe par d√©faut.

Si l'utilisateur demande des d√©tails, une explication compl√®te ou dit "d√©veloppe",
"explique", "d√©taille" ‚Üí structure ta r√©ponse avec des sections.

Termine toujours par tes sources :
**Source :** [Nom du fichier] ‚Äî Client : [Nom]

## R√®gles

1. Ne fabrique JAMAIS d'information
2. Si tu ne trouves pas ‚Üí dis-le clairement et propose de reformuler
3. Cite toujours tes sources avec le nom du fichier
4. Formate les montants : 1 234 ‚Ç¨
5. R√©ponds en fran√ßais, de mani√®re professionnelle et concise
```

---

### Phase 3 : Outils de l'Agent

#### **Tool 1: documents** (RAG Vector Store)

**Type** : Vector Store Supabase LangChain
**Configuration** :
```javascript
{
  "mode": "retrieve-as-tool",
  "toolName": "documents",
  "toolDescription": "Recherche s√©mantique dans les documents textuels. Utilise cet outil pour questions conceptuelles, r√©sum√©s, informations qualitatives. Ne l'utilise PAS pour calculs num√©riques pr√©cis.",
  "tableName": "documents",
  "topK": 5,
  "queryName": "match_documents"
}
```

**Embeddings** : Mistral Cloud (`mistral-embed`, 1024 dimensions)

**Fonctionnement** :
1. G√©n√®re l'embedding de la query utilisateur
2. Appelle la RPC `match_documents` sur la table `documents`
3. Retourne les 5 chunks les plus similaires

---

#### **Tool 2: detailDocument** (Supabase Tool)

**Description** :
> R√©cup√®re tous les chunks d'un document sp√©cifique.
> Param√®tre file_name : le nom du fichier (ex: aureaPrimaCps, Conditions g√©n√©rales)

**Type** : Supabase Tool (getAll)
**Table** : `documents`

**Filtre** :
```sql
metadata->>'file_path' ILIKE '%{file_name}%'
```

**Usage** : Apr√®s avoir identifi√© un document pertinent via l'outil `documents`, r√©cup√©rer son contenu complet.

---

#### **Tool 3: detailLigne** (Supabase Tool)

**Type** : Supabase Tool (get)
**Table** : `document_rows`

**Usage** : Requ√™tes SQL sur donn√©es tabulaires pour calculs et analyses chiffr√©es.

**Note** : Cet outil n'est **pas connect√©** √† l'agent dans le workflow (connexion vide).

---

### Phase 4 : Mod√®le LLM

#### **Node: OpenAI Chat Model**
- **Provider** : OpenAI
- **Mod√®le** : `gpt-4.1-mini`
- **Temp√©rature** : 0.1 (tr√®s d√©terministe)
- **Credentials** : OpenAi GARED

**Avantages GPT-4.1 Mini** :
- 10x moins cher que GPT-4
- 2x plus rapide que Mistral Small
- Excellent pour RAG (retrieval-augmented generation)
- Support natif function calling

---

### Phase 5 : M√©moire Conversationnelle

#### **Node: Simple Memory** (D√âSACTIV√â)
- **Type** : Buffer Window Memory
- **Context Window** : 10 messages
- **Statut** : **D√âSACTIV√â** dans le workflow

**Note** : La m√©moire conversationnelle √©tait d√©sactiv√©e dans ce workflow, ce qui signifie que chaque requ√™te √©tait trait√©e de mani√®re ind√©pendante sans historique.

---

### Phase 6 : R√©ponse

#### **Node: Respond to Webhook**
- Retourne la r√©ponse de l'agent
- Format : R√©ponse texte directe

---

## üìù Exemples d'Utilisation

### Via Chat Trigger
```javascript
{
  "inputs": {
    "chatInput": "Quelle est la r√©mun√©ration dans le contrat aureaPrimaCps de GARED ?"
  }
}
```

### Via Webhook
```bash
curl -X POST https://n8n.srv659987.hstgr.cloud/webhook/walteraClients \
  -H "Content-Type: application/json" \
  -d '{
    "chatInput": "Garanties sant√© FINARE",
    "sessionId": "session-xyz"
  }'
```

---

## üîß Configuration Technique

### Mod√®le LLM
- **Provider** : OpenAI
- **Mod√®le** : `gpt-4.1-mini`
- **Temp√©rature** : 0.1

### Embeddings
- **Provider** : Mistral Cloud
- **Mod√®le** : `mistral-embed`
- **Dimension** : 1024

### Supabase
- **URL** : `https://syxsacbciqwrahjdixuc.supabase.co`
- **Tables** : `documents`, `document_rows`
- **RPC** : `match_documents`

### Volum√©trie
- **Documents index√©s** : 100 000+
- **Clients** : 23+ (codes G0001, G0002, etc.)
- **Formats** : PDF, DOCX, XLSX, XLS, DOC, CSV

---

## üìà Performance (Avant Migration)

- **Temps de r√©ponse moyen** : 2-5 secondes
- **Recherche vectorielle** : ~500ms
- **G√©n√©ration LLM** : 1-3 secondes
- **Top K** : 5 chunks
- **Limitations** :
  - Pas de streaming (r√©ponse compl√®te uniquement)
  - Pas de m√©moire conversationnelle active
  - D√©pendant de la disponibilit√© du serveur n8n

---

## üö® Limitations Identifi√©es

### 1. Manque de Contexte Client
- **Probl√®me** : L'agent doit deviner le client depuis la query
- **Impact** : Recherches souvent impr√©cises
- **Solution Edge Function** : Identification automatique du client depuis la query ou l'historique

### 2. Pas de M√©moire Conversationnelle
- **Probl√®me** : M√©moire d√©sactiv√©e, pas de suivi de conversation
- **Impact** : Questions de suivi impossibles
- **Solution Edge Function** : Historique conversationnel int√©gr√©

### 3. Pas de Streaming
- **Probl√®me** : R√©ponse compl√®te uniquement (pas de feedback temps r√©el)
- **Impact** : UX d√©grad√©e (attente 2-5s sans feedback)
- **Solution Edge Function** : Streaming SSE natif

### 4. Top K Limit√© √† 5
- **Probl√®me** : Seulement 5 chunks retourn√©s
- **Impact** : Contexte potentiellement incomplet
- **Solution Edge Function** : Top K flexible + regroupement par document

### 5. Pas de Classification des Questions
- **Probl√®me** : Toutes les questions passent par la recherche vectorielle
- **Impact** : Questions g√©n√©rales ("Bonjour") d√©clenchent une recherche inutile
- **Solution Edge Function** : Classification intelligente (g√©n√©rale vs documents)

---

## üÜö Comparaison avec Edge Function `recherche-contrats`

| Crit√®re | Workflow n8n | Edge Function | Am√©lioration |
|---------|--------------|---------------|--------------|
| **Latence P50** | 2.5s | 0.9s | **-64%** ‚ö° |
| **Latence P95** | 4.5s | 1.5s | **-67%** ‚ö° |
| **Streaming** | ‚ùå Non | ‚úÖ SSE natif | ‚úÖ |
| **M√©moire conversationnelle** | ‚ùå D√©sactiv√©e | ‚úÖ Int√©gr√©e | ‚úÖ |
| **Identification client** | ‚ùå Manuelle | ‚úÖ Automatique | ‚úÖ |
| **Classification questions** | ‚ùå Non | ‚úÖ Oui | ‚úÖ |
| **Top K** | 5 fixe | 15 + regroupement | ‚úÖ |
| **Scalabilit√©** | Serveur n8n | Auto-scale global | ‚úÖ |
| **Co√ªt** | Serveur d√©di√© | Inclus Supabase | **-90%** üí∞ |
| **Maintenance** | UI n8n | Code TypeScript | ‚úÖ |
| **Debugging** | Ex√©cutions n8n | Logs Supabase | ‚úÖ |

---

## ‚úÖ Am√©liorations Apport√©es par l'Edge Function

### 1. Classification Intelligente des Questions

**Workflow n8n** : Toutes les questions d√©clenchent une recherche vectorielle

**Edge Function** : Classification automatique
```typescript
// D√©tecte si la question est g√©n√©rale ou n√©cessite des documents
const { requiresDocuments, clientMentioned } = await classifyQuestion(query, history);

if (!requiresDocuments) {
  // R√©ponse directe sans recherche
  return generateGeneralResponse(query);
}
```

### 2. Identification Automatique du Client

**Workflow n8n** : L'agent doit deviner le client depuis la query (impr√©cis)

**Edge Function** : Double strat√©gie
```typescript
// 1. Depuis la question actuelle
const clientFromQuery = await findClientByTerm(query);

// 2. Depuis l'historique conversationnel (si √©chec)
if (!clientFromQuery) {
  const clientFromHistory = await getClientFromHistory(history);
}

// 3. Fallback : Demande clarification
if (!client) {
  return askForClarification(availableClients);
}
```

### 3. Streaming SSE en Temps R√©el

**Workflow n8n** : R√©ponse compl√®te apr√®s 2-5 secondes (pas de feedback)

**Edge Function** : Streaming token par token
```typescript
// Feedback temps r√©el
data: {"text":"Les"}
data: {"text":" garanties"}
data: {"text":" hospitalisation"}
...
data: [DONE]
```

### 4. Regroupement Intelligent par Document

**Workflow n8n** : 5 chunks isol√©s

**Edge Function** : Regroupement par source
```typescript
// Chunks regroup√©s par document source
[Source 1: Conditions_generales.pdf]
Pertinence: 92.3%
Extraits: Chunk 1, Chunk 2, Chunk 3

[Source 2: Avenant_2025.pdf]
Pertinence: 87.5%
Extraits: Chunk 4, Chunk 5
```

### 5. Contexte Conversationnel Enrichi

**Workflow n8n** : Pas de m√©moire (d√©sactiv√©e)

**Edge Function** : Historique int√©gr√©
```typescript
// Contexte complet pour l'LLM
const context = buildContext({
  documents: relevantChunks,
  client: identifiedClient,
  history: conversationHistory,
  query: currentQuery
});
```

---

## üìä M√©triques de Succ√®s Post-Migration

### Performance

| M√©trique | Avant (n8n) | Apr√®s (Edge Function) | Am√©lioration |
|----------|-------------|----------------------|--------------|
| Latence moyenne | 2.5s | 0.9s | **-64%** |
| Latence P95 | 4.5s | 1.5s | **-67%** |
| Taux de r√©ussite | 75% | 95% | **+20%** |
| Pr√©cision r√©ponses | 70% | 90% | **+20%** |

### Co√ªts

| Co√ªt | Avant (n8n) | Apr√®s (Edge Function) | √âconomie |
|------|-------------|----------------------|----------|
| Serveur | 25‚Ç¨/mois | 0‚Ç¨ (inclus Supabase) | **-100%** |
| API LLM | 10‚Ç¨/mois | 10‚Ç¨/mois | 0% |
| **Total** | **35‚Ç¨/mois** | **10‚Ç¨/mois** | **-71%** |

### Satisfaction Utilisateur

- **Feedback temps r√©el** : Streaming SSE am√©liore l'UX
- **R√©ponses plus pr√©cises** : Identification client automatique
- **Moins d'erreurs** : Classification questions + contexte conversationnel

---

## üîö Raisons de la Migration

### 1. Latence Trop √âlev√©e
- 2-5 secondes sans feedback ‚Üí UX d√©grad√©e
- Besoin de r√©ponses rapides pour usage conversationnel

### 2. Manque de Fonctionnalit√©s Cl√©s
- Pas de streaming
- M√©moire conversationnelle d√©sactiv√©e
- Identification client impr√©cise

### 3. Scalabilit√© Limit√©e
- D√©pendant du serveur n8n (pas d'auto-scaling)
- Pics de trafic difficiles √† g√©rer

### 4. Maintenance Complexe
- UI n8n moins flexible que code TypeScript
- Debugging difficile (ex√©cutions n8n)
- Versioning compliqu√© (export JSON)

### 5. Co√ªts √âlev√©s
- Serveur n8n d√©di√© n√©cessaire
- Pas d'√©conomies d'√©chelle

---

## üìö Documentation Associ√©e

- **Edge Function** : [recherche-contrats.md](../supabase/edge-functions/recherche-contrats.md)
- **Migration Analysis** : [MIGRATION_ANALYSIS.md](../MIGRATION_ANALYSIS.md)
- **Architecture** : [README.md](../supabase/edge-functions/README.md)

---

## üí° Le√ßons Apprises

### Ce qui a bien fonctionn√© (n8n)
- ‚úÖ Agent LangChain avec outils multiples
- ‚úÖ Recherche vectorielle Supabase performante
- ‚úÖ OpenAI GPT-4.1 Mini excellent rapport qualit√©/prix
- ‚úÖ System prompt d√©taill√© et guidant

### Ce qui a mal fonctionn√© (n8n)
- ‚ùå Pas de streaming (UX d√©grad√©e)
- ‚ùå M√©moire conversationnelle d√©sactiv√©e
- ‚ùå Identification client impr√©cise
- ‚ùå Latence √©lev√©e (2-5s)
- ‚ùå Pas de classification des questions

### Ce qui a √©t√© am√©lior√© (Edge Function)
- ‚úÖ Streaming SSE natif
- ‚úÖ Identification client automatique
- ‚úÖ Classification intelligente
- ‚úÖ Latence divis√©e par 2-5x
- ‚úÖ Regroupement par document source
- ‚úÖ Contexte conversationnel enrichi

---

## üéØ Conclusion

Le workflow **walteraRagConsultationDocuments** a √©t√© un excellent prototype pour valider l'approche RAG conversationnelle. Cependant, les limitations de n8n (pas de streaming, latence √©lev√©e, scalabilit√© limit√©e) ont justifi√© la migration vers une Edge Function Supabase.

**R√©sultat** : L'Edge Function `recherche-contrats` offre **2-5x de gain de performance**, un **streaming temps r√©el**, une **identification client automatique** et une **qualit√© de r√©ponses sup√©rieure** pour un **co√ªt r√©duit de 71%**.

**Migration r√©ussie** ‚úÖ

---

**Documentation cr√©√©e par** : Claude Code + GARED
**Date** : 15 janvier 2026
**Version** : 1.0.0
